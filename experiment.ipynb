{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing this script, I referenced\n",
    " Hayes, G. (2019). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python. https://github.com/gkhayes/mlrose \n",
    " and also https://github.com/hiive/mlrose\n",
    "\n",
    "mlrose is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. This notebook contains the examples used in the mlrose tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "import mlrose_hiive as mlrose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:  Fill the Knapsack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self,n, problem_name, algs, alg_names):\n",
    "        self.n = n\n",
    "        self.name = problem_name\n",
    "        fig1, ax1 = plt.subplots()\n",
    "#         ax1.title = \"Score Per Iteration\"\n",
    "        ax1.set_ylabel(\"Fitness score\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        self.fig1 = fig1\n",
    "        self.ax1 = ax1\n",
    "        \n",
    "        fig2, ax2 = plt.subplots()\n",
    "#         ax2.title = \"Evals Per Iteration\"\n",
    "        ax2.set_ylabel(\"Fitness Evals\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        self.fig2 = fig2\n",
    "        self.ax2 = ax2\n",
    "        \n",
    "        self.algs = algs\n",
    "        self.names = alg_names\n",
    "        self.times = []\n",
    "        self.best_fitnesses = []\n",
    "        \n",
    "    def run(self):\n",
    "        for algorithm, name in zip(self.algs,self.names):\n",
    "            print(f'starting {name}....')\n",
    "            tic = time.perf_counter()\n",
    "            best_state, best_fitness, fitness_curve = algorithm()\n",
    "            toc = time.perf_counter()\n",
    "#             print(f\"{name} completed in {toc - tic:0.4f} seconds\")\n",
    "#             print(f'{name} found best fitness/value: ', best_fitness, '\\n')\n",
    "            \n",
    "            self.times.append(toc-tic)\n",
    "            self.best_fitnesses.append(best_fitness)\n",
    "            self.plot_curve(fitness_curve, f'{name}')\n",
    "        \n",
    "        self.fig1.legend(loc=4)\n",
    "        self.fig2.legend(loc=4)\n",
    "        self.fig1.savefig(f'charts/{self.name}_{n}_items.png', bbox_inches='tight')\n",
    "        self.fig2.savefig(f'charts/{self.name}_{n}_items_evals.png', bbox_inches='tight')\n",
    "        self.fig1.clf()\n",
    "        self.fig2.clf()\n",
    "    def print_stats(self):\n",
    "        for name, time, best_fitness in zip(self.names, self.times, self.best_fitnesses):\n",
    "            print(f'{name} had \\n\\tTime: {time:0.4f} seconds\\n\\tFitness Score: {best_fitness}')\n",
    "    def plot_curve(self, fitness_curve, name):\n",
    "        self.ax1.plot(range(0,len(fitness_curve)), fitness_curve[:,0], label=f'{name}')\n",
    "        self.ax2.plot(range(0,len(fitness_curve)), fitness_curve[:,1], label=f'{name}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting n = 50...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0015 seconds\n",
      "\tFitness Score: 734.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0019 seconds\n",
      "\tFitness Score: 801.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.5517 seconds\n",
      "\tFitness Score: 1148.0\n",
      "mimic had \n",
      "\tTime: 0.3290 seconds\n",
      "\tFitness Score: 1131.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 100...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0009 seconds\n",
      "\tFitness Score: 1477.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0010 seconds\n",
      "\tFitness Score: 1476.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.8313 seconds\n",
      "\tFitness Score: 2235.0\n",
      "mimic had \n",
      "\tTime: 0.8857 seconds\n",
      "\tFitness Score: 2205.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 200...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0032 seconds\n",
      "\tFitness Score: 3278.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0014 seconds\n",
      "\tFitness Score: 2712.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.9519 seconds\n",
      "\tFitness Score: 4216.0\n",
      "mimic had \n",
      "\tTime: 2.5704 seconds\n",
      "\tFitness Score: 4113.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 400...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0028 seconds\n",
      "\tFitness Score: 5875.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0023 seconds\n",
      "\tFitness Score: 5554.0\n",
      "genetic_algorithm had \n",
      "\tTime: 1.8449 seconds\n",
      "\tFitness Score: 8593.0\n",
      "mimic had \n",
      "\tTime: 10.0547 seconds\n",
      "\tFitness Score: 7999.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 800...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0064 seconds\n",
      "\tFitness Score: 11770.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0081 seconds\n",
      "\tFitness Score: 11897.0\n",
      "genetic_algorithm had \n",
      "\tTime: 3.1267 seconds\n",
      "\tFitness Score: 16494.0\n",
      "mimic had \n",
      "\tTime: 41.4258 seconds\n",
      "\tFitness Score: 14972.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 1000...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0106 seconds\n",
      "\tFitness Score: 16274.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0114 seconds\n",
      "\tFitness Score: 15835.0\n",
      "genetic_algorithm had \n",
      "\tTime: 3.2088 seconds\n",
      "\tFitness Score: 22048.0\n",
      "mimic had \n",
      "\tTime: 76.2279 seconds\n",
      "\tFitness Score: 19765.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 1500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0129 seconds\n",
      "\tFitness Score: 22175.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0159 seconds\n",
      "\tFitness Score: 22169.0\n",
      "genetic_algorithm had \n",
      "\tTime: 5.6459 seconds\n",
      "\tFitness Score: 31844.0\n",
      "mimic had \n",
      "\tTime: 228.8529 seconds\n",
      "\tFitness Score: 27529.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize fitness function object using pre-defined class\n",
    "'''\n",
    "Fitness function for Knapsack optimization problem. Given a set of n\n",
    "items, where item i has known weight :math:`w_{i}` and known value\n",
    ":math:`v_{i}`; and maximum knapsack capacity, :math:`W`, the Knapsack\n",
    "fitness function evaluates the fitness of a state vector\n",
    ":math:`x = [x_{0}, x_{1}, \\ldots, x_{n-1}]` as:\n",
    "'''\n",
    "# https://en.wikipedia.org/wiki/Knapsack_problem\n",
    "# We're trying to get to the highest value possible, without going over our weight limit\n",
    "\n",
    "ns = [50,100, 200, 400, 800, 1000,1500] # items in our knapsack\n",
    "weights = []\n",
    "values = []\n",
    "fitnesses = []\n",
    "init_states = []\n",
    "for n in ns:\n",
    "    weight = (np.random.randint(1,50,size=(n)))\n",
    "    value = (np.random.randint(1,50,size=(n)))\n",
    "    init_state = np.random.randint(0,1,size=(n))\n",
    "    init_states.append(init_state)\n",
    "    max_weight_pct = 0.6 # so we can only hold 60% of our total weight for items we're trying to fit\n",
    "    fitnesses.append(mlrose.Knapsack(weight, value, max_weight_pct))\n",
    "    weights.append(weight)\n",
    "    values.append(value)\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 5\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "for weight, value, fitness, n, init_state in zip(weights, values, fitnesses, ns, init_states):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    algs = []\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "\n",
    "    schedule = mlrose.GeomDecay(.01,0.0001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem, schedule=schedule, curve=True, random_state = random_state))\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, random_state=random_state))\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    " \n",
    "\n",
    "    thing = Runner(n, 'knapsack', algs, alg_names)\n",
    "    thing.run()\n",
    "    thing.print_stats()\n",
    "    \n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    print('\\n')\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/knapsack_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/knapsack_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Six Peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting n = 50...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 100...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 200...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 300...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 800...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 1000...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 1500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n"
     ]
    }
   ],
   "source": [
    "# Initialize fitness function object using pre-defined class\n",
    "\n",
    "ns = [50,100,200, 300, 500, 800,1000,1500] # items in our knapsack\n",
    "\n",
    "fitnesses = []\n",
    "for n in ns:\n",
    "    fitnesses.append(mlrose.SixPeaks(t_pct=0.20))\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 5\n",
    "\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "    \n",
    "for fitness, n in zip(fitnesses, ns):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs = []\n",
    "    \n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "    schedule = mlrose.GeomDecay(.01,0.001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem,schedule=schedule, curve=True, random_state = random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, random_state=random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    "    \n",
    "    thing = Runner(n, 'six_peaks', algs, alg_names)\n",
    "    thing.run()\n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    print('\\n')\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/six_peaks_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/six_peaks_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Queens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ns = [50,100,200,400,500,800] # items in our knapsack\n",
    "\n",
    "fitnesses = []\n",
    "init_coords = []\n",
    "for n in ns:\n",
    "    # Define alternative N-Queens fitness function for maximization problem\n",
    "    # This code was ripped off from https://mlrose.readthedocs.io/en/stable/source/tutorial1.html#solving-optimization-problems-with-mlrose\n",
    "    def queens_max(state):\n",
    "       # Initialize counter\n",
    "        fitness_cnt = 0\n",
    "          # For all pairs of queens\n",
    "        for i in range(len(state) - 1):\n",
    "            for j in range(i + 1, len(state)):\n",
    "                # Check for horizontal, diagonal-up and diagonal-down attacks\n",
    "                if (state[j] != state[i]) and (state[j] != state[i] + (j - i)) and (state[j] != state[i] - (j - i)):\n",
    "                   # If no attacks, then increment counter\n",
    "                    fitness_cnt += 1\n",
    "        return fitness_cnt\n",
    "\n",
    "    # Initialize custom fitness function object\n",
    "    fitness_cust = mlrose.CustomFitness(queens_max)\n",
    "    fitnesses.append(fitness_cust)\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 5\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "    \n",
    "for fitness, n in zip(fitnesses, ns):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs = []\n",
    "    \n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "\n",
    "    \n",
    "    schedule = mlrose.GeomDecay(.01,0.001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem, curve=True, random_state = random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, max_iters=100, random_state=random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    "    \n",
    "    thing = Runner(n, 'queens', algs, alg_names)\n",
    "    thing.run()\n",
    "    thing.print_stats()\n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    print('\\n')\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/queens_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/queens_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Fitting a Neural Network to the Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]\n",
    " # https://www.kaggle.com/ronitf/heart-disease-uci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode target values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(np.array(y_train).reshape(-1, 1)).todense()\n",
    "y_test_hot = one_hot.transform(np.array(y_test).reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNRunner:\n",
    "    def __init__(self, alg_names):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_ylabel(\"Fitness score\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        self.fig1 = fig1\n",
    "        self.ax1 = ax1\n",
    "        \n",
    "        fig2, ax2 = plt.subplots()\n",
    "        ax2.set_ylabel(\"Fitness Evals\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        self.fig2 = fig2\n",
    "        self.ax2 = ax2\n",
    "        \n",
    "        self.names = alg_names\n",
    "        self.name = \"NN\"\n",
    "        self.times = []\n",
    "        self.scores = []\n",
    "        \n",
    "        \n",
    "    # Copied from https://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html\n",
    "    def benchmark_influence(self, conf):\n",
    "        \"\"\"\n",
    "        Benchmark influence of `changing_param` on both MSE and latency.\n",
    "        \"\"\"\n",
    "        prediction_times = []\n",
    "        prediction_powers = []\n",
    "        complexities = []\n",
    "        for param_value in conf['changing_param_values']:\n",
    "            conf['tuned_params'][conf['changing_param']] = param_value\n",
    "            estimator = conf['estimator'](**conf['tuned_params'])\n",
    "\n",
    "            print(\"Benchmarking %s\" % estimator)\n",
    "            estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n",
    "            conf['postfit_hook'](estimator)\n",
    "            complexity = conf['complexity_computer'](estimator)\n",
    "            complexities.append(complexity)\n",
    "            start_time = time.time()\n",
    "            for _ in range(conf['n_samples']):\n",
    "                y_pred = estimator.predict(conf['data']['X_test'])\n",
    "            elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n",
    "            prediction_times.append(elapsed_time)\n",
    "            pred_score = conf['prediction_performance_computer'](\n",
    "                conf['data']['y_test'], y_pred)\n",
    "            prediction_powers.append(pred_score)\n",
    "            print(\"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\" % (\n",
    "                complexity, conf['prediction_performance_label'], pred_score,\n",
    "                elapsed_time))\n",
    "        return prediction_powers, prediction_times, complexities\n",
    "    \n",
    "    def _count_nonzero_coefficients(estimator):\n",
    "        a = estimator.coef_.toarray()\n",
    "        return np.count_nonzero(a)\n",
    "\n",
    "\n",
    "    configurations = [\n",
    "        {'estimator': SGDClassifier,\n",
    "         'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':\n",
    "                          'modified_huber', 'fit_intercept': True, 'tol': 1e-3},\n",
    "         'changing_param': 'l1_ratio',\n",
    "         'changing_param_values': [0.25, 0.5, 0.75, 0.9],\n",
    "         'complexity_label': 'non_zero coefficients',\n",
    "         'complexity_computer': _count_nonzero_coefficients,\n",
    "         'prediction_performance_computer': hamming_loss,\n",
    "         'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',\n",
    "         'postfit_hook': lambda x: x.sparsify(),\n",
    "         'data': classification_data,\n",
    "         'n_samples': 30},\n",
    "        {'estimator': NuSVR,\n",
    "         'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},\n",
    "         'changing_param': 'nu',\n",
    "         'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "         'complexity_label': 'n_support_vectors',\n",
    "         'complexity_computer': lambda x: len(x.support_vectors_),\n",
    "         'data': regression_data,\n",
    "         'postfit_hook': lambda x: x,\n",
    "         'prediction_performance_computer': mean_squared_error,\n",
    "         'prediction_performance_label': 'MSE',\n",
    "         'n_samples': 30},\n",
    "        {'estimator': GradientBoostingRegressor,\n",
    "         'tuned_params': {'loss': 'squared_error'},\n",
    "         'changing_param': 'n_estimators',\n",
    "         'changing_param_values': [10, 50, 100, 200, 500],\n",
    "         'complexity_label': 'n_trees',\n",
    "         'complexity_computer': lambda x: x.n_estimators,\n",
    "         'data': regression_data,\n",
    "         'postfit_hook': lambda x: x,\n",
    "         'prediction_performance_computer': mean_squared_error,\n",
    "         'prediction_performance_label': 'MSE',\n",
    "         'n_samples': 30},\n",
    "    ]\n",
    "    def run(self):\n",
    "        for name in self.names:\n",
    "            print(f'starting {name}....')\n",
    "            tic = time.perf_counter()\n",
    "            print('===============================================')\n",
    "            print(f'running NN weight optimization for {name}')\n",
    "            # Initialize neural network object and fit object - attempt 1\n",
    "\n",
    "\n",
    "            schedule = mlrose.GeomDecay(.01,0.0001) \n",
    "            nn = mlrose.NeuralNetwork(hidden_nodes = [2], activation ='relu', \n",
    "                                             algorithm =name, \n",
    "                                             max_iters = 100, bias = False, is_classifier = True, \n",
    "                                             learning_rate = 0.0001, early_stopping = True,\n",
    "                                             schedule=schedule,\n",
    "                                             curve=True,\n",
    "                                             clip_max = 5, max_attempts = 1000, random_state = 5)\n",
    "            _, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "            axes.set_title(\"NN\")\n",
    "            axes.set_xlabel(\"Training examples\")\n",
    "            axes.set_ylabel(\"Score\")\n",
    "            train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "                    learning_curve(nn, X_train_scaled, y_train_hot,return_times=True)\n",
    "\n",
    "            train_scores_mean = np.mean(train_scores, axis=1)\n",
    "            train_scores_std = np.std(train_scores, axis=1)\n",
    "            test_scores_mean = np.mean(test_scores, axis=1)\n",
    "            test_scores_std = np.std(test_scores, axis=1)\n",
    "            fit_times_mean = np.mean(fit_times, axis=1)\n",
    "            fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "            # Plot learning curve\n",
    "            axes.grid()\n",
    "            axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                                 color=\"r\")\n",
    "            axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                                 test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                                 color=\"g\")\n",
    "            axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                         label=\"Training score\")\n",
    "            axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                         label=\"Cross-validation score\")\n",
    "            axes.legend(loc=\"best\")\n",
    "\n",
    "            plt.savefig(f'charts/NN_{self.name}_{name}.png', bbox_inches='tight') \n",
    "            plt.clf()\n",
    "        \n",
    "        self.fig1.legend(loc=4)\n",
    "        self.fig2.legend(loc=4)\n",
    "        self.fig1.savefig(f'charts/{self.name}_{n}_items.png', bbox_inches='tight')\n",
    "        self.fig2.savefig(f'charts/{self.name}_{n}_items_evals.png', bbox_inches='tight')\n",
    "        self.fig1.clf()\n",
    "        self.fig2.clf()\n",
    "    \n",
    "    def print_stats(self):\n",
    "        for name, time, best_fitness in zip(self.names, self.times, self.scores):\n",
    "            print(f'{name} had \\n\\tTime: {time:0.4f} seconds\\n\\t Score: {score}')\n",
    "    def plot_curve(self, fitness_curve, name):\n",
    "        if len(fitness_curve.shape) == 2:\n",
    "            self.ax1.plot(range(0,len(fitness_curve)), fitness_curve[:,0], label=f'{name}')\n",
    "            self.ax2.plot(range(0,len(fitness_curve)), fitness_curve[:,1], label=f'{name}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['random_hill_climb', 'simulated_annealing', 'genetic_alg','gradient_descent']\n",
    "\n",
    "runner_for_nn = NNRunner(algorithms)\n",
    "\n",
    "runner_for_nn.run()\n",
    "\n",
    "runner_for_nn.print_stats()\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdd7e1c2594becc0f0730fd88e427749c52dec6ea015258dec14119a99cf0656"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
