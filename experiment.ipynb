{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Assignment 2: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing this script, I referenced\n",
    " Hayes, G. (2019). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python. https://github.com/gkhayes/mlrose \n",
    " and also https://github.com/hiive/mlrose\n",
    "\n",
    "mlrose is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. This notebook contains the examples used in the mlrose tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "import mlrose_hiive as mlrose\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:  Fill the Knapsack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self,n, problem_name, algs, alg_names):\n",
    "        self.n = n\n",
    "        self.name = problem_name\n",
    "        fig1, ax1 = plt.subplots()\n",
    "#         ax1.title = \"Score Per Iteration\"\n",
    "        ax1.set_ylabel(\"Fitness score\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        self.fig1 = fig1\n",
    "        self.ax1 = ax1\n",
    "        \n",
    "        fig2, ax2 = plt.subplots()\n",
    "#         ax2.title = \"Evals Per Iteration\"\n",
    "        ax2.set_ylabel(\"Fitness Evals\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        self.fig2 = fig2\n",
    "        self.ax2 = ax2\n",
    "        \n",
    "        self.algs = algs\n",
    "        self.names = alg_names\n",
    "        self.times = []\n",
    "        self.best_fitnesses = []\n",
    "        \n",
    "    def run(self):\n",
    "        for algorithm, name in zip(self.algs,self.names):\n",
    "            print(f'starting {name}....')\n",
    "            tic = time.perf_counter()\n",
    "            best_state, best_fitness, fitness_curve = algorithm()\n",
    "            toc = time.perf_counter()\n",
    "#             print(f\"{name} completed in {toc - tic:0.4f} seconds\")\n",
    "#             print(f'{name} found best fitness/value: ', best_fitness, '\\n')\n",
    "            \n",
    "            self.times.append(toc-tic)\n",
    "            self.best_fitnesses.append(best_fitness)\n",
    "            self.plot_curve(fitness_curve, f'{name}')\n",
    "        \n",
    "        self.fig1.legend(loc=4)\n",
    "        self.fig2.legend(loc=4)\n",
    "        self.fig1.savefig(f'charts/{self.name}_{n}_items.png', bbox_inches='tight')\n",
    "        self.fig2.savefig(f'charts/{self.name}_{n}_items_evals.png', bbox_inches='tight')\n",
    "        self.fig1.clf()\n",
    "        self.fig2.clf()\n",
    "    def print_stats(self):\n",
    "        for name, time, best_fitness in zip(self.names, self.times, self.best_fitnesses):\n",
    "            print(f'{name} had \\n\\tTime: {time:0.4f} seconds\\n\\tFitness Score: {best_fitness}')\n",
    "    def plot_curve(self, fitness_curve, name):\n",
    "        self.ax1.plot(range(0,len(fitness_curve)), fitness_curve[:,0], label=f'{name}')\n",
    "        self.ax2.plot(range(0,len(fitness_curve)), fitness_curve[:,1], label=f'{name}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting n = 50...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0009 seconds\n",
      "\tFitness Score: 691.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0011 seconds\n",
      "\tFitness Score: 756.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.5807 seconds\n",
      "\tFitness Score: 1188.0\n",
      "mimic had \n",
      "\tTime: 0.2196 seconds\n",
      "\tFitness Score: 1174.0\n",
      "=====================\n",
      "starting n = 100...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0009 seconds\n",
      "\tFitness Score: 1596.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0011 seconds\n",
      "\tFitness Score: 1627.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.9303 seconds\n",
      "\tFitness Score: 2213.0\n",
      "mimic had \n",
      "\tTime: 0.8448 seconds\n",
      "\tFitness Score: 2181.0\n",
      "=====================\n",
      "starting n = 200...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0018 seconds\n",
      "\tFitness Score: 3038.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0012 seconds\n",
      "\tFitness Score: 2751.0\n",
      "genetic_algorithm had \n",
      "\tTime: 1.4482 seconds\n",
      "\tFitness Score: 4422.0\n",
      "mimic had \n",
      "\tTime: 3.0438 seconds\n",
      "\tFitness Score: 4164.0\n",
      "=====================\n",
      "starting n = 400...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0026 seconds\n",
      "\tFitness Score: 5654.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0018 seconds\n",
      "\tFitness Score: 5473.0\n",
      "genetic_algorithm had \n",
      "\tTime: 2.0055 seconds\n",
      "\tFitness Score: 8280.0\n",
      "mimic had \n",
      "\tTime: 10.6733 seconds\n",
      "\tFitness Score: 7780.0\n",
      "=====================\n",
      "starting n = 800...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0060 seconds\n",
      "\tFitness Score: 11941.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0076 seconds\n",
      "\tFitness Score: 11959.0\n",
      "genetic_algorithm had \n",
      "\tTime: 4.5015 seconds\n",
      "\tFitness Score: 17220.0\n",
      "mimic had \n",
      "\tTime: 46.2030 seconds\n",
      "\tFitness Score: 15665.0\n",
      "=====================\n",
      "starting n = 1000...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0083 seconds\n",
      "\tFitness Score: 14912.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0091 seconds\n",
      "\tFitness Score: 14662.0\n",
      "genetic_algorithm had \n",
      "\tTime: 4.0156 seconds\n",
      "\tFitness Score: 21028.0\n",
      "mimic had \n",
      "\tTime: 73.9072 seconds\n",
      "\tFitness Score: 18923.0\n",
      "=====================\n",
      "starting n = 1500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0155 seconds\n",
      "\tFitness Score: 23116.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0170 seconds\n",
      "\tFitness Score: 22844.0\n",
      "genetic_algorithm had \n",
      "\tTime: 3.1802 seconds\n",
      "\tFitness Score: 31377.0\n",
      "mimic had \n",
      "\tTime: 183.3023 seconds\n",
      "\tFitness Score: 28497.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize fitness function object using pre-defined class\n",
    "'''\n",
    "Fitness function for Knapsack optimization problem. Given a set of n\n",
    "items, where item i has known weight :math:`w_{i}` and known value\n",
    ":math:`v_{i}`; and maximum knapsack capacity, :math:`W`, the Knapsack\n",
    "fitness function evaluates the fitness of a state vector\n",
    ":math:`x = [x_{0}, x_{1}, \\ldots, x_{n-1}]` as:\n",
    "'''\n",
    "# https://en.wikipedia.org/wiki/Knapsack_problem\n",
    "# We're trying to get to the highest value possible, without going over our weight limit\n",
    "\n",
    "ns = [50,100, 200, 400, 800, 1000,1500] # items in our knapsack\n",
    "weights = []\n",
    "values = []\n",
    "fitnesses = []\n",
    "init_states = []\n",
    "for n in ns:\n",
    "    weight = (np.random.randint(1,50,size=(n)))\n",
    "    value = (np.random.randint(1,50,size=(n)))\n",
    "    init_state = np.random.randint(0,1,size=(n))\n",
    "    init_states.append(init_state)\n",
    "    max_weight_pct = 0.6 # so we can only hold 60% of our total weight for items we're trying to fit\n",
    "    fitnesses.append(mlrose.Knapsack(weight, value, max_weight_pct))\n",
    "    weights.append(weight)\n",
    "    values.append(value)\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 5\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "for weight, value, fitness, n, init_state in zip(weights, values, fitnesses, ns, init_states):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    algs = []\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "\n",
    "    schedule = mlrose.GeomDecay(.01,0.0001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem, schedule=schedule, curve=True, random_state = random_state))\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, random_state=random_state))\n",
    "    \n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    " \n",
    "\n",
    "    thing = Runner(n, 'knapsack', algs, alg_names)\n",
    "    thing.run()\n",
    "    thing.print_stats()\n",
    "    \n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/knapsack_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/knapsack_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Six Peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting n = 50...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0005 seconds\n",
      "\tFitness Score: 6.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0077 seconds\n",
      "\tFitness Score: 76.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.7112 seconds\n",
      "\tFitness Score: 83.0\n",
      "mimic had \n",
      "\tTime: 0.1949 seconds\n",
      "\tFitness Score: 16.0\n",
      "=====================\n",
      "starting n = 100...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0002 seconds\n",
      "\tFitness Score: 1.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0959 seconds\n",
      "\tFitness Score: 63.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.8143 seconds\n",
      "\tFitness Score: 136.0\n",
      "mimic had \n",
      "\tTime: 0.6473 seconds\n",
      "\tFitness Score: 22.0\n",
      "=====================\n",
      "starting n = 200...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0003 seconds\n",
      "\tFitness Score: 4.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.3968 seconds\n",
      "\tFitness Score: 99.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.2483 seconds\n",
      "\tFitness Score: 24.0\n",
      "mimic had \n",
      "\tTime: 2.5044 seconds\n",
      "\tFitness Score: 27.0\n",
      "=====================\n",
      "starting n = 300...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0004 seconds\n",
      "\tFitness Score: 4.0\n",
      "simulated_annealing had \n",
      "\tTime: 1.0237 seconds\n",
      "\tFitness Score: 150.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.5254 seconds\n",
      "\tFitness Score: 39.0\n",
      "mimic had \n",
      "\tTime: 4.0822 seconds\n",
      "\tFitness Score: 25.0\n",
      "=====================\n",
      "starting n = 400...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0003 seconds\n",
      "\tFitness Score: 7.0\n",
      "simulated_annealing had \n",
      "\tTime: 2.3583 seconds\n",
      "\tFitness Score: 190.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.5613 seconds\n",
      "\tFitness Score: 30.0\n",
      "mimic had \n",
      "\tTime: 11.7623 seconds\n",
      "\tFitness Score: 27.0\n",
      "=====================\n",
      "starting n = 500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0002 seconds\n",
      "\tFitness Score: 1.0\n",
      "simulated_annealing had \n",
      "\tTime: 1.9534 seconds\n",
      "\tFitness Score: 202.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.8081 seconds\n",
      "\tFitness Score: 46.0\n",
      "mimic had \n",
      "\tTime: 18.7856 seconds\n",
      "\tFitness Score: 33.0\n",
      "=====================\n",
      "starting n = 600...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0002 seconds\n",
      "\tFitness Score: 1.0\n",
      "simulated_annealing had \n",
      "\tTime: 4.9119 seconds\n",
      "\tFitness Score: 246.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.5122 seconds\n",
      "\tFitness Score: 32.0\n",
      "mimic had \n",
      "\tTime: 17.0309 seconds\n",
      "\tFitness Score: 29.0\n",
      "=====================\n",
      "starting n = 700...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0006 seconds\n",
      "\tFitness Score: 2.0\n",
      "simulated_annealing had \n",
      "\tTime: 5.8429 seconds\n",
      "\tFitness Score: 278.0\n",
      "genetic_algorithm had \n",
      "\tTime: 1.0137 seconds\n",
      "\tFitness Score: 64.0\n",
      "mimic had \n",
      "\tTime: 67.3983 seconds\n",
      "\tFitness Score: 34.0\n",
      "=====================\n",
      "starting n = 800...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0005 seconds\n",
      "\tFitness Score: 1.0\n",
      "simulated_annealing had \n",
      "\tTime: 16.8229 seconds\n",
      "\tFitness Score: 345.0\n",
      "genetic_algorithm had \n",
      "\tTime: 0.7305 seconds\n",
      "\tFitness Score: 41.0\n",
      "mimic had \n",
      "\tTime: 81.9621 seconds\n",
      "\tFitness Score: 18.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize fitness function object using pre-defined class\n",
    "\n",
    "ns = [50,100,200, 300,400, 500,600,700,800] # items in our knapsack\n",
    "\n",
    "fitnesses = []\n",
    "for n in ns:\n",
    "    fitnesses.append(mlrose.SixPeaks(t_pct=0.20))\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 5\n",
    "\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "    \n",
    "for fitness, n in zip(fitnesses, ns):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs = []\n",
    "    \n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "    schedule = mlrose.GeomDecay(.01,0.001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem,schedule=schedule, curve=True, random_state = random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, random_state=random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    "    \n",
    "    thing = Runner(n, 'six_peaks', algs, alg_names)\n",
    "    thing.run()\n",
    "    thing.print_stats()\n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/six_peaks_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/six_peaks_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Queens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "starting n = 50...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0373 seconds\n",
      "\tFitness Score: 606.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.0614 seconds\n",
      "\tFitness Score: 607.0\n",
      "genetic_algorithm had \n",
      "\tTime: 2.6137 seconds\n",
      "\tFitness Score: 612.0\n",
      "mimic had \n",
      "\tTime: 2.9115 seconds\n",
      "\tFitness Score: 618.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 100...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 0.0920 seconds\n",
      "\tFitness Score: 2450.0\n",
      "simulated_annealing had \n",
      "\tTime: 0.3310 seconds\n",
      "\tFitness Score: 2473.0\n",
      "genetic_algorithm had \n",
      "\tTime: 20.3773 seconds\n",
      "\tFitness Score: 2470.0\n",
      "mimic had \n",
      "\tTime: 15.2176 seconds\n",
      "\tFitness Score: 2480.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 200...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 1.3346 seconds\n",
      "\tFitness Score: 9929.0\n",
      "simulated_annealing had \n",
      "\tTime: 1.2893 seconds\n",
      "\tFitness Score: 9924.0\n",
      "genetic_algorithm had \n",
      "\tTime: 68.7527 seconds\n",
      "\tFitness Score: 9931.0\n",
      "mimic had \n",
      "\tTime: 63.9665 seconds\n",
      "\tFitness Score: 9957.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 400...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 6.9365 seconds\n",
      "\tFitness Score: 39840.0\n",
      "simulated_annealing had \n",
      "\tTime: 7.7648 seconds\n",
      "\tFitness Score: 39843.0\n",
      "genetic_algorithm had \n",
      "\tTime: 438.7826 seconds\n",
      "\tFitness Score: 39841.0\n",
      "mimic had \n",
      "\tTime: 307.9254 seconds\n",
      "\tFitness Score: 39882.0\n",
      "\n",
      "\n",
      "=====================\n",
      "starting n = 500...\n",
      "starting random_hill_climbing....\n",
      "starting simulated_annealing....\n",
      "starting genetic_algorithm....\n",
      "starting mimic....\n",
      "random_hill_climbing had \n",
      "\tTime: 7.7455 seconds\n",
      "\tFitness Score: 62265.0\n",
      "simulated_annealing had \n",
      "\tTime: 9.0169 seconds\n",
      "\tFitness Score: 62269.0\n",
      "genetic_algorithm had \n",
      "\tTime: 326.2535 seconds\n",
      "\tFitness Score: 62290.0\n",
      "mimic had \n",
      "\tTime: 481.5403 seconds\n",
      "\tFitness Score: 62351.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns = [50,100,200,400,500] # items in our knapsack\n",
    "\n",
    "fitnesses = []\n",
    "init_coords = []\n",
    "for n in ns:\n",
    "    # Define alternative N-Queens fitness function for maximization problem\n",
    "    # This code was ripped off from https://mlrose.readthedocs.io/en/stable/source/tutorial1.html#solving-optimization-problems-with-mlrose\n",
    "    def queens_max(state):\n",
    "       # Initialize counter\n",
    "        fitness_cnt = 0\n",
    "          # For all pairs of queens\n",
    "        for i in range(len(state) - 1):\n",
    "            for j in range(i + 1, len(state)):\n",
    "                # Check for horizontal, diagonal-up and diagonal-down attacks\n",
    "                if (state[j] != state[i]) and (state[j] != state[i] + (j - i)) and (state[j] != state[i] - (j - i)):\n",
    "                   # If no attacks, then increment counter\n",
    "                    fitness_cnt += 1\n",
    "        return fitness_cnt\n",
    "\n",
    "    # Initialize custom fitness function object\n",
    "    fitness_cust = mlrose.CustomFitness(queens_max)\n",
    "    fitnesses.append(fitness_cust)\n",
    "\n",
    "\n",
    "# basically have a seed state so we can reproduce\n",
    "random_state = 15\n",
    "fitness_values= dict()\n",
    "times = dict()\n",
    "alg_names = ['random_hill_climbing','simulated_annealing','genetic_algorithm','mimic']\n",
    "for name in alg_names:\n",
    "    fitness_values[name] = []\n",
    "    times[name] = []\n",
    "    \n",
    "for fitness, n in zip(fitnesses, ns):\n",
    "    print('=====================')\n",
    "    print(f'starting n = {n}...')\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs = []\n",
    "    \n",
    "    algs.append(lambda: mlrose.random_hill_climb(problem, curve=True, random_state = random_state))\n",
    "\n",
    "    \n",
    "    schedule = mlrose.GeomDecay(.01,0.001)\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.simulated_annealing(problem, curve=True, random_state = random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    algs.append(lambda: mlrose.genetic_alg(problem, curve=True, max_iters=100, random_state=random_state))\n",
    "    problem = mlrose.DiscreteOpt(length = n, fitness_fn = fitness, maximize=True, max_val=2)\n",
    "    problem.set_mimic_fast_mode(True)\n",
    "    algs.append(lambda: mlrose.mimic(problem,\n",
    "                                     curve=True, \n",
    "                                     random_state=random_state))\n",
    "    \n",
    "    thing = Runner(n, 'queens', algs, alg_names)\n",
    "    thing.run()\n",
    "    thing.print_stats()\n",
    "    for index, name in enumerate(alg_names):\n",
    "        fitness_values[name].append(thing.best_fitnesses[index])\n",
    "        times[name].append(thing.times[index])\n",
    "    print('\\n')\n",
    "    \n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Fitness score\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, fitness_values[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/queens_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"Seconds to Convergence\")\n",
    "ax1.set_xlabel(\"Problem Size\")\n",
    "\n",
    "for name in alg_names:\n",
    "    ax1.plot(ns, times[name], label=name)\n",
    "\n",
    "fig1.legend(loc=4)\n",
    "fig1.savefig(f'charts/queens_times_problem_size.png', bbox_inches='tight')\n",
    "fig1.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Fitting a Neural Network to the Heart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/heart.csv\")\n",
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]\n",
    " # https://www.kaggle.com/ronitf/heart-disease-uci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                    random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode target values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "y_train_hot = one_hot.fit_transform(np.array(y_train).reshape(-1, 1)).todense()\n",
    "y_test_hot = one_hot.transform(np.array(y_test).reshape(-1, 1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNRunner:\n",
    "    def __init__(self, alg_names):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_ylabel(\"Fitness score\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        self.fig1 = fig1\n",
    "        self.ax1 = ax1\n",
    "        \n",
    "        fig2, ax2 = plt.subplots()\n",
    "        ax2.set_ylabel(\"Fitness Evals\")\n",
    "        ax2.set_xlabel(\"Iterations\")\n",
    "        self.fig2 = fig2\n",
    "        self.ax2 = ax2\n",
    "        \n",
    "        self.names = alg_names\n",
    "        self.name = \"NN\"\n",
    "        self.times = []\n",
    "        self.scores = []\n",
    "        \n",
    "        \n",
    "    # Copied from https://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html\n",
    "    def benchmark_influence(self, conf):\n",
    "        \"\"\"\n",
    "        Benchmark influence of `changing_param` on both MSE and latency.\n",
    "        \"\"\"\n",
    "        prediction_times = []\n",
    "        prediction_powers = []\n",
    "        complexities = []\n",
    "        for param_value in conf['changing_param_values']:\n",
    "            conf['tuned_params'][conf['changing_param']] = param_value\n",
    "            estimator = conf['estimator'](**conf['tuned_params'])\n",
    "\n",
    "            print(\"Benchmarking %s\" % estimator)\n",
    "            estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n",
    "            conf['postfit_hook'](estimator)\n",
    "            complexity = conf['complexity_computer'](estimator)\n",
    "            complexities.append(complexity)\n",
    "            start_time = time.time()\n",
    "            for _ in range(conf['n_samples']):\n",
    "                y_pred = estimator.predict(conf['data']['X_test'])\n",
    "            elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n",
    "            prediction_times.append(elapsed_time)\n",
    "            pred_score = conf['prediction_performance_computer'](\n",
    "                conf['data']['y_test'], y_pred)\n",
    "            prediction_powers.append(pred_score)\n",
    "            print(\"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\" % (\n",
    "                complexity, conf['prediction_performance_label'], pred_score,\n",
    "                elapsed_time))\n",
    "        return prediction_powers, prediction_times, complexities\n",
    "    \n",
    "    def count_nonzero_coefficients(self,estimator):\n",
    "        a = estimator.coef_.toarray()\n",
    "        return np.count_nonzero(a)\n",
    "\n",
    "    \n",
    "    def plot_influence(self,conf, mse_values, prediction_times, complexities):\n",
    "        \"\"\"\n",
    "        Plot influence of model complexity on both accuracy and latency.\n",
    "        \"\"\"\n",
    "\n",
    "        fig = plt.figure()\n",
    "        fig.subplots_adjust(right=0.75)\n",
    "\n",
    "        # first axes (prediction error)\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        line1 = ax1.plot(complexities, mse_values, c='tab:blue', ls='-')[0]\n",
    "        ax1.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])\n",
    "        y1_label = conf['prediction_performance_label']\n",
    "        ax1.set_ylabel(y1_label)\n",
    "\n",
    "        ax1.spines['left'].set_color(line1.get_color())\n",
    "        ax1.yaxis.label.set_color(line1.get_color())\n",
    "        ax1.tick_params(axis='y', colors=line1.get_color())\n",
    "\n",
    "        # second axes (latency)\n",
    "        ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n",
    "        line2 = ax2.plot(complexities, prediction_times, c='tab:orange', ls='-')[0]\n",
    "        ax2.yaxis.tick_right()\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        y2_label = \"Time (s)\"\n",
    "        ax2.set_ylabel(y2_label)\n",
    "        ax1.spines['right'].set_color(line2.get_color())\n",
    "        ax2.yaxis.label.set_color(line2.get_color())\n",
    "        ax2.tick_params(axis='y', colors=line2.get_color())\n",
    "\n",
    "        plt.legend((line1, line2), (\"prediction error\", \"latency\"),\n",
    "                   loc='upper right')\n",
    "\n",
    "        plt.title(\"Influence of varying '%s' on %s\" % (conf['changing_param'],\n",
    "                                                       conf['estimator'].__name__))\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for name in self.names:\n",
    "            print(f'starting {name}....')\n",
    "            tic = time.perf_counter()\n",
    "            print('===============================================')\n",
    "            print(f'running NN weight optimization for {name}')\n",
    "            # Initialize neural network object and fit object - attempt 1\n",
    "\n",
    "\n",
    "            schedule = mlrose.GeomDecay(.01,0.0001) \n",
    "            nn = mlrose.NeuralNetwork(hidden_nodes = [2], activation ='relu', \n",
    "                                             algorithm =name, \n",
    "                                             max_iters = 100, bias = False, is_classifier = True, \n",
    "                                             learning_rate = 0.0001, early_stopping = True,\n",
    "                                             schedule=schedule,\n",
    "                                             restarts=10,\n",
    "                                             curve=True,\n",
    "                                             clip_max = 5, max_attempts = 1000, random_state = 5)\n",
    "            _, axes = plt.subplots()\n",
    "            axes.set_title(\"NN\")\n",
    "            axes.set_xlabel(\"Training examples\")\n",
    "            axes.set_ylabel(\"Score\")\n",
    "            train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "                    learning_curve(nn, X_train_scaled, y_train_hot,return_times=True)\n",
    "\n",
    "            train_scores_mean = np.mean(train_scores, axis=1)\n",
    "            train_scores_std = np.std(train_scores, axis=1)\n",
    "            test_scores_mean = np.mean(test_scores, axis=1)\n",
    "            test_scores_std = np.std(test_scores, axis=1)\n",
    "            fit_times_mean = np.mean(fit_times, axis=1)\n",
    "            fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "            # Plot learning curve\n",
    "            axes.grid()\n",
    "            axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                                 color=\"r\")\n",
    "            axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                                 test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                                 color=\"g\")\n",
    "            axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                         label=\"Training score\")\n",
    "            axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                         label=\"Cross-validation score\")\n",
    "            axes.legend(loc=\"best\")\n",
    "\n",
    "            plt.savefig(f'charts/NN_{self.name}_{name}.png', bbox_inches='tight') \n",
    "            plt.clf()\n",
    "            \n",
    "            tic = time.perf_counter()\n",
    "            nn.fit(X_train_scaled, y_train_hot)\n",
    "            toc = time.perf_counter()\n",
    "            y_test_pred = nn.predict(X_test_scaled)\n",
    "            \n",
    "            y_test_accuracy = accuracy_score(y_test_hot, y_test_pred)\n",
    "            \n",
    "            print(\"accuracy is:\")\n",
    "            print(y_test_accuracy)\n",
    "            print(f'Took : {toc-tic:0.4f} seconds')\n",
    "        \n",
    "        self.fig1.legend(loc=4)\n",
    "        self.fig2.legend(loc=4)\n",
    "        self.fig1.savefig(f'charts/{self.name}_{n}_items.png', bbox_inches='tight')\n",
    "        self.fig2.savefig(f'charts/{self.name}_{n}_items_evals.png', bbox_inches='tight')\n",
    "        self.fig1.clf()\n",
    "        self.fig2.clf()\n",
    "    \n",
    "    def plot_curve(self, fitness_curve, name):\n",
    "        if len(fitness_curve.shape) == 2:\n",
    "            self.ax1.plot(range(0,len(fitness_curve)), fitness_curve[:,0], label=f'{name}')\n",
    "            self.ax2.plot(range(0,len(fitness_curve)), fitness_curve[:,1], label=f'{name}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting random_hill_climb....\n",
      "===============================================\n",
      "running NN weight optimization for random_hill_climb\n",
      "accuracy is:\n",
      "0.5573770491803278\n",
      "Took : 1.1436 seconds\n",
      "starting simulated_annealing....\n",
      "===============================================\n",
      "running NN weight optimization for simulated_annealing\n",
      "accuracy is:\n",
      "0.5573770491803278\n",
      "Took : 0.1427 seconds\n",
      "starting genetic_alg....\n",
      "===============================================\n",
      "running NN weight optimization for genetic_alg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:\n",
      "0.8360655737704918\n",
      "Took : 14.7721 seconds\n",
      "starting gradient_descent....\n",
      "===============================================\n",
      "running NN weight optimization for gradient_descent\n",
      "accuracy is:\n",
      "0.7213114754098361\n",
      "Took : 0.1382 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "algorithms = ['random_hill_climb', 'simulated_annealing', 'genetic_alg','gradient_descent']\n",
    "from sklearn.metrics import mean_squared_error\n",
    "nn = NNRunner(algorithms)\n",
    "\n",
    "classification_data =  {'X_train': X_train_scaled, 'X_test': X_test_scaled, 'y_train': y_train_hot, 'y_test': y_test_hot}\n",
    "configurations = [\n",
    "    {\n",
    "        'estimator': mlrose.NeuralNetwork,\n",
    "        'tuned_params': \n",
    "            {\n",
    "                'learning_rate': 0.01 ,\n",
    "                'hidden_nodes' : [2],\n",
    "                'algorithm': 'genetic_alg',\n",
    "                'activation': 'relu',\n",
    "                'is_classifier': True,\n",
    "                'early_stopping': True,\n",
    "                'curve': True,\n",
    "                'clip_max': 5,\n",
    "                'max_attempts': 1000,\n",
    "                'max_iters': 1000,\n",
    "                'bias': False,\n",
    "                'random_state': 5\n",
    "            },\n",
    "         'changing_param': 'hidden_nodes',\n",
    "         'changing_param_values': [[2],[6],[8],[50],[100],[200]],\n",
    "         'complexity_label': 'hidden nodes',\n",
    "         'complexity_computer': lambda x: np.prod(x.hidden_nodes),\n",
    "         'prediction_performance_computer': mean_squared_error,\n",
    "         'prediction_performance_label': 'MSE',\n",
    "         'postfit_hook': lambda x: x,\n",
    "         'data': classification_data,\n",
    "         'n_samples': len(X_test_scaled)\n",
    "    },\n",
    "        {\n",
    "        'estimator': mlrose.NeuralNetwork,\n",
    "        'tuned_params': \n",
    "            {\n",
    "                'learning_rate': 0.01 ,\n",
    "                'hidden_nodes' : [2],\n",
    "                'algorithm': 'random_hill_climb',\n",
    "                'activation': 'relu',\n",
    "                'is_classifier': True,\n",
    "                'early_stopping': True,\n",
    "                'curve': True,\n",
    "                'clip_max': 5,\n",
    "                'max_attempts': 1000,\n",
    "                'max_iters': 1000,\n",
    "                'bias': False,\n",
    "                'random_state': 5\n",
    "            },\n",
    "         'changing_param': 'hidden_nodes',\n",
    "         'changing_param_values': [[2],[6],[8],[50],[100],[200]],\n",
    "         'complexity_label': 'hidden nodes',\n",
    "         'complexity_computer': lambda x: np.prod(x.hidden_nodes),\n",
    "         'prediction_performance_computer': mean_squared_error,\n",
    "         'prediction_performance_label': 'MSE',\n",
    "         'postfit_hook': lambda x: x,\n",
    "         'data': classification_data,\n",
    "         'n_samples': len(X_test_scaled)\n",
    "    },\n",
    "    {\n",
    "    'estimator': mlrose.NeuralNetwork,\n",
    "    'tuned_params': \n",
    "        {\n",
    "            'learning_rate': 0.01 ,\n",
    "            'hidden_nodes' : [2],\n",
    "            'algorithm': 'simulated_annealing',\n",
    "            'activation': 'relu',\n",
    "            'is_classifier': True,\n",
    "            'early_stopping': True,\n",
    "            'curve': True,\n",
    "            'clip_max': 5,\n",
    "            'max_attempts': 1000,\n",
    "            'max_iters': 1000,\n",
    "            'bias': False,\n",
    "            'random_state': 5\n",
    "        },\n",
    "     'changing_param': 'hidden_nodes',\n",
    "     'changing_param_values': [[2],[6],[8],[50],[100],[200]],\n",
    "     'complexity_label': 'hidden nodes',\n",
    "     'complexity_computer': lambda x: np.prod(x.hidden_nodes),\n",
    "     'prediction_performance_computer': mean_squared_error,\n",
    "     'prediction_performance_label': 'MSE',\n",
    "     'postfit_hook': lambda x: x,\n",
    "     'data': classification_data,\n",
    "     'n_samples': len(X_test_scaled)\n",
    "},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for conf in configurations:\n",
    "    prediction_performances, prediction_times, complexities = nn.benchmark_influence(conf)\n",
    "    nn.plot_influence(conf, prediction_performances, prediction_times, complexities)\n",
    "    namey = conf['estimator'].__name__\n",
    "    alg = conf['tuned_params']['algorithm']\n",
    "    plt.savefig(f'charts/NN_{namey}_{alg}_complexity.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "nn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdd7e1c2594becc0f0730fd88e427749c52dec6ea015258dec14119a99cf0656"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
